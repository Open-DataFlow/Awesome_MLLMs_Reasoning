# Awesome_MLLMs_Reasoning

In this repository, we will continuously update the latest papers, slides, and other valuable resources that advance MLLM reasoning, making learning more efficient for everyone!

<!-- omit in toc -->
## üì¢ Updates

- **2025.03**: We released this repo. Feel free to cite or open pull requests.

<!-- omit in toc -->
## üìö Table of Contents
- [Awesome_MLLMs_Reasoning](#-awesome_mllms_reasoning)
  - 
  - [1.Technical Report](#-1technical-report)
  - [2.Generated Data Guided Post-Training](#-2generated-data-guided-post-training)
  - [3.Test-time Scaling](#-3test-time-scaling)
  - [4.Benchmarks](#-4benchmarks)
- [Open-Source Projects](#Ô∏è-open-source-projects)



## üìñ Papers

### üìù  1.Technical Report
We also feature some well-known technical reports on Large Language Models (LLMs) reasoning.
* [2503] [QwQ-32B: Embracing the Power of Reinforcement Learning](https://qwenlm.github.io/blog/qwq-32b/) (Qwen Team)(https://img.shields.io/badge/github-2025.03-red)
* [2501] [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
Reinforcement Learning](https://arxiv.org/pdf/2501.12948) (DeepSeek Team)
* [2501] [KIMI K1.5: SCALING REINFORCEMENT LEARNING WITH LLMS](https://arxiv.org/pdf/2501.12599) (Kimi Team)(https://img.shields.io/badge/github-2025.03-red)

### üìå 2.Generated Data Guided Post-Training
* [2503] [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785) (Shanghai AI Lab)   [Code](https://github.com/Liuziyu77/Visual-RFT)

* [2502] [OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference](https://arxiv.org/pdf/2502.18411) (Shanghai AI Lab) [Code](https://github.com/PhoenixZ810/OmniAlign-V)

* [2502] [MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification](https://arxiv.org/pdf/2502.13383) (PKU) [Code](https://github.com/Aurora-slz/MM-Verify)

* [2502] [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/pdf/2502.10391) (Kuaishou) [Code](https://github.com/Kwai-YuanQi/MM-RLHF)

* [2501] [Can We Generate Images with CoT? Let‚Äôs Verify and Reinforce Image Generation Step by Step](https://arxiv.org/pdf/2501.13926) (CUHK) [Code](https://github.com/ZiyuGuo99/Image-Generation-CoT)

* [2501] [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/pdf/2501.04686) (ByteDance)

* [2501] [LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs](https://arxiv.org/pdf/2501.06186) (Mohamed bin Zayed University of AI)

* [2501] [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/pdf/2501.07542) (Microsoft Research)

* [2501] [Technical Report on Slow Thinking with LLMs: Visual Reasoning](https://arxiv.org/pdf/2501.01904) (Renmin University of China)

* [2412] [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/pdf/2412.05237) (CMU)

* [2412] [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704) (University of Maryland)

* [2412] [TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action](https://arxiv.org/pdf/2412.05479) (University of Washington)

* [2412] [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/pdf/2412.17451) (HKUST)

* [2412] [Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/pdf/2412.14835) 

* [2411] [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/pdf/2411.10442) 

* [2411] [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/pdf/2411.18203) 

* [2411] [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/pdf/2411.14432) 

* [2411] [AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning](https://arxiv.org/pdf/2411.11930) 

* [2411] [LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/pdf/2411.10440v1) 

* [2411] [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://arxiv.org/pdf/2411.00855) 

* [2403] [Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models](https://arxiv.org/pdf/2403.16999) 

* [2306] [Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic](https://arxiv.org/pdf/2306.15195) 

[‚¨ÜÔ∏è Back to Top](#-table-of-contents)

### üöÄ 3.Test-time Scaling
* [2502] [Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking](https://arxiv.org/pdf/2502.02339) 

* [2502] [MLLMS KNOW WHERE TO LOOK: TRAINING-FREE PERCEPTION OF SMALL VISUAL DETAILS WITH MULTIMODAL LLMS](https://arxiv.org/pdf/2502.17422) 

* [2412] [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704) 

* [2412] [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704) 

* [2409] [Visual Agents as Fast and Slow Thinkers](https://openreview.net/pdf?id=ncCuiD3KJQ) 

* [2411] [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://arxiv.org/pdf/2411.00855) [Code](https://github.com/njucckevin/MM-Self-Improve)

* [2402] [Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models](https://arxiv.org/pdf/2402.12058) [Code](https://github.com/leixy20/Scaffold)

* [2402] [V-STaR: Training Verifiers for Self-Taught Reasoners](https://arxiv.org/pdf/2402.06457) 

[‚¨ÜÔ∏è Back to Top](#-table-of-contents)
### üìä 4.Benchmarks

* [2502] [Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models](https://arxiv.org/pdf/2502.14191) [Code](https://github.com/facebookresearch/multimodal_rewardbench)  

* [2502] [ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models](https://arxiv.org/pdf/2502.09696) [Code](https://zerobench.github.io/)  

* [2502] [MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models](https://arxiv.org/pdf/2502.00698) [Code](https://acechq.github.io/MMIQ-benchmark/)  

* [2502] [MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency](https://arxiv.org/pdf/2502.09621) [Code](https://mmecot.github.io/)

* [2406] [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/pdf/2406.16860) [Code](https://github.com/cambrian-mllm/cambrian)

* [2404] [BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/pdf/2404.12390) 

* [2401] [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/pdf/2401.06209) 

* [2312] [V‚àó: Guided Visual Search as a Core Mechanism in Multimodal LLMs](https://arxiv.org/pdf/2312.14135)   

[‚¨ÜÔ∏è Back to Top](#-table-of-contents)

## üõ†Ô∏è Open-Source Projects
* [R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3] [Code](https://github.com/Deep-Agent/R1-V)![](https://img.shields.io/badge/github-2025.02-red)

* [EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework] [Code](https://github.com/hiyouga/EasyR1)![](https://img.shields.io/badge/github-2025.02-red)  

* [R1-OnevisionÔºöAn Open-Source Multimodal Large Language Model Capable of Deep Reasoning] [Code](https://github.com/Fancy-MLLM/R1-Onevision)![](https://img.shields.io/badge/github-2025.02-red)  

* [LMM-R1] [Code](https://github.com/TideDra/lmm-r1)![](https://img.shields.io/badge/github-2025.02-red)  

* [VLM-R1: A stable and generalizable R1-style Large Vision-Language Model] [Code](https://github.com/om-ai-lab/VLM-R1)![](https://img.shields.io/badge/github-2025.02-red)  

* [Multi-modal Open R1] [Code](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)![](https://img.shields.io/badge/github-2025.02-red)  

* [Video-R1: Towards Super Reasoning Ability in Video Understanding] [Code](https://github.com/tulerfeng/Video-R1)![](https://img.shields.io/badge/github-2025.02-red)  

* [Open-R1-Video] [Code](https://github.com/Wang-Xiaodong1899/Open-R1-Video)![](https://img.shields.io/badge/github-2025.02-red)  

* [R1-Vision: Let's first take a look at the image] [Code](https://github.com/yuyq96/R1-Vision)![](https://img.shields.io/badge/github-2025.02-red)  

[‚¨ÜÔ∏è Back to Top](#-table-of-contents)

## ü§ù Contributing

You‚Äôre welcome to submit new resources or paper links. Please initiate a [Pull Request](https://github.com/your-repo-url/pulls) directly.

## üìú Citation
